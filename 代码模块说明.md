# 基于社会计算的金融诈骗检测机制 - 代码模块说明

本文档按照README.md中列出的功能模块，对代码进行分块解释，便于实验报告撰写。

---

## 一、数据模拟与加载模块

### 1.1 社会计算理论基础：社交网络数据建模

**理论背景**：在社会计算中，社交平台被建模为多实体交互系统，包含用户（User）、内容（Content）和交互（Interaction）三类核心实体。这种建模方式遵循**异构图（Heterogeneous Graph）**理论，能够捕捉不同类型节点和边的语义关系。

**形式化定义**：
- 用户集合：$U = \{u_1, u_2, ..., u_n\}$，每个用户具有属性向量 $\mathbf{p}_u = [age, followers, following, verified, bot\_score]$
- 内容集合：$C = \{c_1, c_2, ..., c_m\}$，每条内容包含文本 $t_c$、作者 $a(c) \in U$、时间戳 $\tau_c$
- 交互集合：$I = \{(u_i, u_j, c_k, \tau, type)\}$，表示用户 $u_i$ 对用户 $u_j$ 发布的内容 $c_k$ 在时间 $\tau$ 进行类型为 $type$ 的交互

**社会计算知识点**：
- **幂律分布（Power Law）**：社交网络中的粉丝数遵循Zipf分布，体现"长尾效应"，少数用户拥有大量连接
- **时间序列建模**：交互时间戳 $\tau$ 用于分析信息扩散的时间模式
- **标签分布**：诈骗比例 $fraud\_ratio$ 控制类别不平衡程度，模拟真实场景

### 1.2 `src/data/simulate.py` - 合成数据生成器

**功能概述**：生成模拟社交平台数据，包括用户画像、内容发布和用户交互。

**核心函数**：`simulate_social_platform(num_users, fraud_ratio, seed)`

**主要实现逻辑**：

```33:53:src/data/simulate.py
def simulate_social_platform(num_users: int = 2000, fraud_ratio: float = 0.08, seed: int = 42) -> Dict[str, Any]:
	random.seed(seed)
	np.random.seed(seed)

	# 用户画像
	user_ids = [f"u_{i}" for i in range(num_users)]
	account_age_days = np.random.randint(7, 2000, size=num_users)
	followers = np.random.zipf(2, size=num_users)
	followers = np.clip(followers, 0, 50000)
	following = np.random.poisson(100, size=num_users)
	verified = np.random.binomial(1, 0.05, size=num_users)
	bot_score = np.clip(np.random.beta(2, 8, size=num_users) + verified * 0.05, 0, 1)

	users = pd.DataFrame({
		'user_id': user_ids,
		'account_age_days': account_age_days,
		'followers': followers,
		'following': following,
		'verified': verified,
		'bot_score': bot_score,
	})
```

- **用户画像生成**：生成用户ID、账户年龄、粉丝数（Zipf分布模拟幂律）、关注数（泊松分布）、认证状态、机器人评分
  - **社会计算对应**：Zipf分布体现社交网络的幂律特性，$\text{followers} \sim \text{Zipf}(2)$
- **内容生成**：根据诈骗比例生成正常/诈骗内容，使用预定义关键词库
  - **社会计算对应**：内容标签 $y_c \in \{0, 1\}$ 表示是否为诈骗，遵循伯努利分布 $y_c \sim \text{Bernoulli}(fraud\_ratio)$
- **交互生成**：模拟转发、评论、私信等交互行为，构建有向关系网络
  - **社会计算对应**：交互类型 $type \in \{\text{retweet}, \text{reply}, \text{dm}\}$，概率分布 $P(\text{retweet})=0.6, P(\text{reply})=0.3, P(\text{dm})=0.1$

**输出数据结构**：
- `users`: 用户表（user_id, account_age_days, followers, following, verified, bot_score）
- `contents`: 内容表（post_id, author_id, text, created_at, is_fraud）
- `interactions`: 交互表（src_user, dst_user, post_id, time, type）
- `labels`: 标签表（post_id, is_fraud）

### 1.3 `src/data/dataset.py` - 数据框构建与清洗

**功能概述**：将模拟数据转换为标准DataFrame格式，并进行基本数据清洗。

**核心函数**：`build_dataframe(sim_data)`

```6:18:src/data/dataset.py
def build_dataframe(sim_data: Dict[str, Any]) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:
	users = sim_data['users'].copy()
	contents = sim_data['contents'].copy()
	interactions = sim_data['interactions'].copy()
	labels = sim_data['labels'].copy()

	# 基本清洗
	users.drop_duplicates('user_id', inplace=True)
	contents.drop_duplicates('post_id', inplace=True)
	if not interactions.empty:
		interactions.dropna(subset=['src_user', 'dst_user'], inplace=True)

	return users, contents, interactions, labels
```

**主要操作**：
- 数据复制与去重（用户ID、内容ID）
- 交互数据空值处理

### 1.4 `src/data/loader_uci.py` - UCI数据集加载器

**功能概述**：加载UCI公开数据集（钓鱼网站、短信垃圾），转换为项目统一数据格式。

**核心函数**：
- `load_uci_phishing(base_dir)`: 加载UCI钓鱼网站数据集
- `load_uci_sms_spam(base_dir)`: 加载UCI短信垃圾数据集

**主要实现逻辑**：

```9:36:src/data/loader_uci.py
def load_uci_phishing(base_dir: str) -> Dict[str, Any]:
	"""
	期望文件：
	- Phishing Websites Data Set: 常见文件名 'phishing.csv' 或 'PhishingData.csv' 或包含 30/31 个特征列与 'Result' 标签
	映射策略：
	- 每条样本→一条 content；text 由离散特征拼接为词
	- 无真实用户/关系：生成最小用户表（每条样本一个作者），交互留空
	- 标签：Result（1/-1、phishing/legitimate）→ is_fraud(1 表示诈骗)
	"""
	# 尝试若干常见文件名
	candidates = [
		'phishing.csv', 'PhishingData.csv', 'PhishingWebsitesData.csv', 'data.csv', 'dataset.csv'
	]
	csv_path = None
	for name in candidates:
		p = os.path.join(base_dir, name)
		if os.path.exists(p):
			csv_path = p
			break
	if csv_path is None:
		# 回退：查找目录下首个 .csv
		for fname in os.listdir(base_dir):
			if fname.lower().endswith('.csv'):
				csv_path = os.path.join(base_dir, fname)
				break
	if csv_path is None:
		raise FileNotFoundError('未找到 UCI Phishing 的 CSV 文件，请将解压后的 csv 放在此目录。')
```

**关键处理**：
- **文件自动识别**：支持多种常见文件名和分隔符（逗号、分号、制表符）
- **特征转文本**：将离散特征拼接为文本token，便于文本特征提取
- **标签规范化**：统一转换为is_fraud（0/1）格式
- **虚拟用户生成**：UCI数据不含社交关系，生成最小用户表和空交互表

---

## 二、特征工程模块

### 2.1 社会计算理论基础：多模态特征融合

**理论背景**：在社会计算中，诈骗检测需要融合**内容特征（Content Features）**、**用户画像特征（Profile Features）**、**时间特征（Temporal Features）**和**社会信号特征（Social Signal Features）**。这种多模态融合方法基于**特征工程理论**和**社会网络分析（SNA）**。

**形式化定义**：
最终特征矩阵 $X \in \mathbb{R}^{n \times d}$ 由四部分拼接而成：

$$X = [X_{\text{text}} \oplus X_{\text{user}} \oplus X_{\text{time}} \oplus X_{\text{social}}]$$

其中：
- $X_{\text{text}} \in \mathbb{R}^{n \times d_t}$：文本TF-IDF特征（稀疏矩阵）
- $X_{\text{user}} \in \mathbb{R}^{n \times 5}$：用户画像特征（账户年龄、粉丝数、关注数、认证状态、机器人评分）
- $X_{\text{time}} \in \mathbb{R}^{n \times 2}$：时间特征（小时、星期几）
- $X_{\text{social}} \in \mathbb{R}^{n \times 3}$：社会信号特征（度中心性、PageRank、聚类系数）

**社会计算知识点**：
- **TF-IDF向量化**：捕捉文本中的关键词频率，识别诈骗内容中的异常词汇模式
- **特征标准化**：使用Z-score归一化 $\mathbf{x}' = \frac{\mathbf{x} - \mu}{\sigma}$，消除不同特征量纲的影响
- **特征融合策略**：水平拼接（horizontal stacking）将不同模态特征统一到同一特征空间

### 2.2 `src/features/extract.py` - 多维度特征提取器

**功能概述**：从内容、用户、时间、网络等多个维度提取特征，融合为统一特征矩阵。

**核心类**：`FeatureExtractor`

**特征类型**：

#### 2.2.1 文本特征（TF-IDF）

```19:21:src/features/extract.py
	def _text_features(self, df_contents: pd.DataFrame) -> sparse.csr_matrix:
		texts = df_contents['text'].fillna('').astype(str).tolist()
		return self.vectorizer.fit_transform(texts)
```

- 使用TF-IDF向量化，支持1-2元组（ngram_range）
  - **社会计算对应**：TF-IDF权重 $w_{t,d} = \text{tf}(t,d) \times \log\frac{N}{\text{df}(t)}$，捕捉诈骗内容中的高频异常词汇
- 最大特征数可配置（默认5000）
- 输出稀疏矩阵格式

#### 2.2.2 用户特征

```23:36:src/features/extract.py
	def _user_features(self, df_users: pd.DataFrame, df_contents: pd.DataFrame) -> np.ndarray:
		uid_to_row = {u: i for i, u in enumerate(df_users['user_id'].tolist())}
		author_idx = df_contents['author_id'].map(uid_to_row).fillna(-1).astype(int).values
		u = df_users
		feat = np.stack([
			u['account_age_days'].values,
			u['followers'].values,
			u['following'].values,
			u['verified'].values,
			u['bot_score'].values,
		], axis=1)
		feat = (feat - feat.mean(0)) / (feat.std(0) + 1e-6)
		post_feat = feat[np.clip(author_idx, 0, len(feat)-1)]
		return post_feat
```

- 提取5维用户画像：账户年龄、粉丝数、关注数、认证状态、机器人评分
  - **社会计算对应**：用户画像 $\mathbf{p}_u$ 反映用户的**社会资本（Social Capital）**和**可信度（Credibility）**
- 通过作者ID映射到内容，每个内容继承其作者的用户特征
  - **社会计算对应**：内容特征继承作者属性，体现**同质性原理（Homophily）**：相似用户发布相似内容
- 特征标准化（Z-score归一化）：$\mathbf{x}' = \frac{\mathbf{x} - \mu}{\sigma}$

#### 2.2.3 时间特征

```38:45:src/features/extract.py
	def _temporal_features(self, df_contents: pd.DataFrame) -> np.ndarray:
		# 简单的时间特征：小时/星期几
		created = pd.to_datetime(df_contents['created_at'])
		hour = created.dt.hour.values.reshape(-1, 1)
		wday = created.dt.weekday.values.reshape(-1, 1)
		feat = np.concatenate([hour, wday], axis=1).astype(float)
		feat = (feat - feat.mean(0)) / (feat.std(0) + 1e-6)
		return feat
```

- 提取发布时间的2维特征：小时（0-23）、星期几（0-6）
  - **社会计算对应**：时间特征捕捉**时间模式（Temporal Patterns）**，诈骗内容通常在特定时间段发布（如深夜、工作日）
- 标准化处理

#### 2.2.4 社会信号特征

```47:55:src/features/extract.py
	def _social_signal_features(self, df_contents: pd.DataFrame, social_signals: pd.DataFrame) -> np.ndarray:
		merged = df_contents[['post_id', 'author_id']].merge(social_signals, on='author_id', how='left')
		cols = ['deg', 'pr', 'clust']
		for c in cols:
			if c not in merged:
				merged[c] = 0.0
		vals = merged[cols].fillna(0.0).values.astype(float)
		vals = (vals - vals.mean(0)) / (vals.std(0) + 1e-6)
		return vals
```

- 融合社会图计算得到的信号：度中心性（deg）、PageRank（pr）、聚类系数（clust）
  - **社会计算对应**：这些信号来自社会网络分析（SNA），用于衡量用户在社交网络中的**影响力（Influence）**和**嵌入性（Embeddedness）**
- 通过作者ID关联，缺失值填充为0

#### 2.2.5 特征融合

```57:77:src/features/extract.py
	def transform(
		self,
		df_users: pd.DataFrame,
		df_contents: pd.DataFrame,
		df_interactions: pd.DataFrame,
		df_labels: pd.DataFrame,
		social_signals: pd.DataFrame,
	) -> Tuple[sparse.csr_matrix, np.ndarray, pd.DataFrame]:
		X_text = self._text_features(df_contents)
		X_user = self._user_features(df_users, df_contents)
		X_time = self._temporal_features(df_contents)
		X_sig = self._social_signal_features(df_contents, social_signals)

		X_dense = np.concatenate([X_user, X_time, X_sig], axis=1)
		X = sparse.hstack([X_text, sparse.csr_matrix(X_dense)], format='csr')

		label_map = df_labels.set_index('post_id')['is_fraud']
		y = df_contents['post_id'].map(label_map).fillna(0).astype(int).values

		meta = df_contents[['post_id', 'author_id', 'created_at']].copy()
		return X, y, meta
```

- 将文本特征（稀疏）与用户/时间/社会信号特征（稠密）水平拼接
- 输出：特征矩阵X（稀疏CSR格式）、标签y、元数据meta

---

## 三、社会信号与图计算模块

### 3.1 社会计算理论基础：社会网络分析与图计算

**理论背景**：在社会计算中，**社会网络分析（Social Network Analysis, SNA）**是核心方法。我们将社交平台数据形式化为一个**有向图（Directed Graph）** $G = (V, E)$，其中：

- $V$ 表示节点集合（用户节点 $U$）
- $E$ 表示边集合（交互关系，如转发、评论、私信）

**形式化定义**：
- **图结构**：$G = (V, E)$，其中 $V = \{u_1, u_2, ..., u_n\}$，$E = \{(u_i, u_j) | \exists \text{ interaction from } u_i \text{ to } u_j\}$
- **邻接矩阵**：$A \in \{0,1\}^{n \times n}$，$A_{ij} = 1$ 表示存在从 $u_i$ 到 $u_j$ 的边

**社会计算知识点**：
- **节点中心性（Node Centrality）**：衡量节点在网络中的重要性
  - **度中心性**：$\text{deg}(u) = |N(u)|$，其中 $N(u)$ 是节点 $u$ 的邻居集合
- **PageRank算法**：基于随机游走的节点重要性评分
  $$PR(u) = \frac{1-d}{N} + d \sum_{v \in M(u)} \frac{PR(v)}{L(v)}$$
  其中 $d=0.85$ 是阻尼系数，$M(u)$ 是指向 $u$ 的节点集合，$L(v)$ 是 $v$ 的出度
- **聚类系数（Clustering Coefficient）**：衡量节点邻居间的连接密度
  $$C(u) = \frac{2e_u}{k_u(k_u-1)}$$
  其中 $e_u$ 是 $u$ 的邻居间实际边数，$k_u$ 是 $u$ 的度数

**群体智慧（Collective Intelligence）**：通过计算社会信号，利用网络结构信息提高检测率，区分核心诈骗节点与边缘节点。

### 3.2 `src/signals/social_graph.py` - 社交图构建与信号计算

**功能概述**：基于用户交互数据构建有向社交图，计算节点级社会信号指标。

#### 3.2.1 图构建

```6:15:src/signals/social_graph.py
def build_social_graph(df_users: pd.DataFrame, df_interactions: pd.DataFrame) -> nx.DiGraph:
	G = nx.DiGraph()
	for _, row in df_users.iterrows():
		G.add_node(row['user_id'])
	if not df_interactions.empty:
		# 以交互构边：src -> dst
		edges = df_interactions[['src_user', 'dst_user']].dropna()
		for _, r in edges.iterrows():
			G.add_edge(r['src_user'], r['dst_user'])
	return G
```

- 使用NetworkX构建有向图（DiGraph）
  - **社会计算对应**：构建异构图 $G = (V, E)$，其中 $V = U$（用户集合），$E$ 由交互数据构建
- 节点：所有用户
- 边：基于交互数据（src_user → dst_user），表示用户间的互动关系
  - **社会计算对应**：边 $(u_i, u_j)$ 表示用户 $u_i$ 对用户 $u_j$ 进行了交互（转发/评论/私信），体现**有向关系（Directed Relationship）**

#### 3.2.2 社会信号计算

```18:38:src/signals/social_graph.py
def compute_social_signals(G: nx.DiGraph, df_users: pd.DataFrame, df_contents: pd.DataFrame, df_interactions: pd.DataFrame) -> pd.DataFrame:
	if len(G) == 0:
		return pd.DataFrame({'author_id': [], 'deg': [], 'pr': [], 'clust': []})

	deg = dict(G.degree())
	try:
		pr = nx.pagerank(G, alpha=0.85)
	except Exception:
		pr = {n: 0.0 for n in G.nodes}
	try:
		clust_u = nx.clustering(G.to_undirected())
	except Exception:
		clust_u = {n: 0.0 for n in G.nodes}

	df = pd.DataFrame({
		'author_id': list(G.nodes),
		'deg': [deg.get(n, 0.0) for n in G.nodes],
		'pr': [pr.get(n, 0.0) for n in G.nodes],
		'clust': [clust_u.get(n, 0.0) for n in G.nodes],
	})
	return df
```

**计算的信号指标**：
- **度中心性（deg）**：节点的总度数（入度+出度），反映用户连接数
  - **社会计算对应**：$\text{deg}(u) = \text{deg}_{in}(u) + \text{deg}_{out}(u)$，用于识别**核心节点（Core Nodes）**和**边缘节点（Peripheral Nodes）**
- **PageRank（pr）**：基于随机游走的节点重要性，alpha=0.85
  - **社会计算对应**：PageRank值反映节点的**影响力（Influence）**，高PageRank用户可能是意见领袖或诈骗团伙核心
- **聚类系数（clust）**：节点邻居间的连接密度（需转为无向图计算）
  - **社会计算对应**：高聚类系数表示用户处于**紧密社区（Dense Community）**中，可能属于诈骗团伙

**异常处理**：计算失败时返回0值，保证程序健壮性

**代码实现对应**：
```python
# main.py - 社会图谱构建模块
from src.data.simulate import simulate_social_platform
from src.data.dataset import build_dataframe
from src.signals.social_graph import build_social_graph, compute_social_signals

# 1. 加载或模拟数据
if args.dataset == "simulated":
    sim_data = simulate_social_platform(**cfg["simulation"])

# 2. 构建 DataFrame（清洗、标准化）
df_users, df_contents, df_interactions, df_labels = build_dataframe(sim_data)

# 3. 构建社会图谱 Graph G = (V, E)
G = build_social_graph(df_users, df_interactions)

# 4. 计算社会信号并加入特征矩阵
social_signals = compute_social_signals(G, df_users, df_contents, df_interactions)
```

这一部分将用户、内容与交互映射为异构图结构，通过 `build_social_graph()` 构建用户交互有向图，并使用 `compute_social_signals()` 计算节点中心性、PageRank与聚类系数等社会信号。

---

## 四、检测模型模块

### 4.1 社会计算理论基础：多模态特征融合与模型训练

**理论背景**：在社会计算中，诈骗检测是一个**二分类问题**，需要学习从多模态特征空间到标签空间的映射函数 $f: X \rightarrow \{0, 1\}$。

**形式化定义**：
- **特征空间**：$X = X_{\text{text}} \oplus X_{\text{user}} \oplus X_{\text{time}} \oplus X_{\text{social}}$
- **标签空间**：$Y = \{0, 1\}$，其中 $0$ 表示正常内容，$1$ 表示诈骗内容
- **学习目标**：最小化损失函数 $L(f(X), Y)$

**社会计算知识点**：
- **多模态特征融合**：特征矩阵 $X$ 由三部分组成：
  - $X_{\text{text}}$：文本特征（TF-IDF稀疏矩阵）
  - $X_{\text{profile}} = X_{\text{user}} \oplus X_{\text{time}}$：用户画像与时间特征
  - $X_{\text{social}}$：社会信号特征（度中心性、PageRank、聚类系数）
- **模型选择策略**：
  - **稀疏特征**：使用LogisticRegression（支持稀疏矩阵，L2正则化）
  - **稠密特征**：使用HistGradientBoostingClassifier（梯度提升树，能学习非线性关系）
  - **单类别场景**：使用IsolationForest（无监督异常检测）

**模型优势**：
- **处理稀疏数据**：LogisticRegression的saga求解器能高效处理TF-IDF稀疏矩阵
- **非线性学习**：梯度提升树能自动学习文本特征与社会信号之间的非线性关系
- **鲁棒性**：支持类别不平衡和单类别场景

### 4.2 `src/models/detector.py` - 诈骗检测器

**功能概述**：训练分类模型进行诈骗检测，支持有监督和无监督两种模式。

**核心类**：`FraudDetector`

#### 4.2.1 模型训练与评估

```16:85:src/models/detector.py
	def fit_and_eval(self, X, y, eval_cfg: Dict[str, Any]):
		indices = np.arange(len(y))
		X_train, X_test, y_train, y_test, train_idx, test_idx = train_test_split(
			X, y, indices,
			test_size=eval_cfg.get('test_size', 0.25),
			random_state=eval_cfg.get('random_state', 42),
			stratify=None if len(np.unique(y)) == 1 else y
		)

		# 单一类别：切换无监督异常检测
		if len(np.unique(y_train)) < 2 or len(np.unique(y_test)) < 2:
			is_sparse = sparse.issparse(X_train)
			Xtr = X_train if not is_sparse else X_train
			Xte = X_test if not is_sparse else X_test
			oc = IsolationForest(n_estimators=200, contamination='auto', random_state=42)
			oc.fit(Xtr)
			# 越异常分数越高，转为 [0,1]
			scores = -oc.score_samples(Xte)
			s_min, s_max = float(np.min(scores)), float(np.max(scores))
			proba = (scores - s_min) / (s_max - s_min + 1e-8)
			metrics = {
				'roc_auc': None,
				'pr_auc': None,
				'roc_curve': ([], []),
				'pr_curve': ([], []),
				'note': 'single_class_subset_used_isolation_forest',
			}
			self.model = oc
			return self.model, metrics, (test_idx, y_test, proba)

		is_sparse = sparse.issparse(X_train)
		params = self.cfg.get('params', {})

		if is_sparse:
			# 稀疏友好：LogisticRegression(saga)
			self.model = LogisticRegression(
				solver='saga',
				penalty='l2',
				C=1.0,
				max_iter=int(params.get('max_iter', 300)),
				n_jobs=-1,
				verbose=0,
			)
			self.model.fit(X_train, y_train)
			proba = self.model.predict_proba(X_test)[:, 1]
		else:
			# 稠密：保持原 HGB
			self.model = HistGradientBoostingClassifier(
				max_depth=None,
				learning_rate=params.get('learning_rate', 0.08),
				max_iter=params.get('n_estimators', 300),
				l2_regularization=1e-4,
				random_state=42,
			)
			X_train_arr = X_train.toarray() if hasattr(X_train, 'toarray') else X_train
			X_test_arr = X_test.toarray() if hasattr(X_test, 'toarray') else X_test
			self.model.fit(X_train_arr, y_train)
			proba = self.model.predict_proba(X_test_arr)[:, 1]

		roc_auc = roc_auc_score(y_test, proba)
		pr_auc = average_precision_score(y_test, proba)
		fpr, tpr, _ = roc_curve(y_test, proba)
		prec, rec, _ = precision_recall_curve(y_test, proba)
		metrics = {
			'roc_auc': float(roc_auc),
			'pr_auc': float(pr_auc),
			'roc_curve': (fpr.tolist(), tpr.tolist()),
			'pr_curve': (prec.tolist(), rec.tolist()),
		}
		return self.model, metrics, (test_idx, y_test, proba)
```

**模型选择策略**：
1. **单类别检测**：使用IsolationForest（无监督异常检测）
2. **稀疏特征**：使用LogisticRegression（saga求解器，支持稀疏矩阵）
3. **稠密特征**：使用HistGradientBoostingClassifier（梯度提升树）

**评估指标**：
- **ROC AUC**：ROC曲线下面积，衡量模型区分正负样本的能力
  - **社会计算对应**：ROC AUC反映模型在**不平衡数据集**上的整体性能
- **PR AUC**：精确率-召回率曲线下面积，更适合不平衡分类问题
  - **社会计算对应**：PR AUC在诈骗检测中更重要，因为诈骗样本通常占少数（$fraud\_ratio = 0.08$）
- 曲线数据：用于后续可视化

**代码实现对应**：
```python
# main.py - 特征提取与模型训练模块
from src.features.extract import FeatureExtractor
from src.models.detector import FraudDetector

# 特征提取模块
fe = FeatureExtractor(cfg["features"])

# 文本、用户属性、社会信号特征的融合
X_text = fe._text_features(df_contents)      # TF-IDF特征
X_profile = fe._user_features(df_users, df_contents)  # 用户画像
X_time = fe._temporal_features(df_contents)   # 时间特征
X_social = fe._social_signal_features(df_contents, social_signals)  # 社会信号

# 最终特征矩阵（论文中提到的公式 X = Xt ⊕ Xp ⊕ Xs）
X = sparse.hstack([X_text, sparse.csr_matrix(np.concatenate([X_profile, X_time, X_social], axis=1))])
y = df_contents["is_fraud"].values

# 模型训练
detector = FraudDetector(cfg["model"])
clf, metrics, (test_idx, y_test, y_pred_proba) = detector.fit_and_eval(X, y, cfg["evaluation"])
```

采用基于梯度的决策树提升算法（HistGradientBoostingClassifier），该模型能有效处理稀疏数据，并能自动学习文本特征与社会信号之间的非线性关系。参数配置为 `n_estimators=300, learning_rate=0.08`。

---

## 五、干预策略与仿真模块

### 5.1 社会计算理论基础：干预策略与社会影响

**理论背景**：在社会计算中，**干预策略（Intervention Strategy）**是指对检测到的高风险内容或用户采取的行动，以减少诈骗信息的传播。这涉及**信息扩散控制（Information Diffusion Control）**和**社会影响最小化（Social Influence Minimization）**。

**形式化定义**：
- **风险评分**：对每条内容 $c$，模型输出风险概率 $P(y_c = 1 | X_c) \in [0, 1]$
- **阈值策略**：标记高风险内容 $F = \{c | P(y_c = 1 | X_c) \geq \theta\}$，其中 $\theta$ 是阈值
- **每日上限**：限制每日最大处置数量 $M$，避免过度干预
- **冷却机制**：对被标记作者 $A_F = \{a(c) | c \in F\}$ 施加冷却期，减少其内容的传播

**社会计算知识点**：
- **阈值决策**：基于**风险评分（Risk Score）**的二元决策，平衡**精确率（Precision）**和**召回率（Recall）**
- **资源约束**：每日上限 $M$ 体现**资源限制（Resource Constraints）**，模拟实际平台的处理能力
- **扩散抑制**：冷却机制基于**信息扩散模型（Information Diffusion Model）**，假设干预可减少50%的扩散量
- **效果评估**：通过统计标记内容数、被标记作者数和估计减少的扩散量来评估干预效果

### 5.2 `src/intervention/strategy.py` - 干预仿真器

**功能概述**：基于模型预测概率，模拟对高风险内容的干预策略及其效果。

**核心类**：`InterventionSimulator`

#### 5.2.1 干预策略

```13:45:src/intervention/strategy.py
	def simulate(self, meta: pd.DataFrame, y_pred_proba: np.ndarray, df_interactions: pd.DataFrame) -> Dict[str, Any]:
		# 基于阈值排序，限制每日最大处置量，对高风险内容的作者施加冷却（减少扩散）
		df = meta.copy()
		df['proba'] = y_pred_proba
		df.sort_values('proba', ascending=False, inplace=True)
		df['flag'] = (df['proba'] >= self.threshold).astype(int)

		# 每日上限控制
		df['date'] = pd.to_datetime(df['created_at']).dt.date
		flagged = []
		for d, group in df.groupby('date'):
			g = group[group['flag'] == 1].head(self.max_daily_flags)
			flagged.append(g)
		flagged = pd.concat(flagged) if len(flagged) else df.head(0)

		flag_authors = set(flagged['author_id'].tolist())
		# 冷却：近cooldown_days内对被标记作者的传播互动减少（仅用于报告统计）
		if not df_interactions.empty:
			inter = df_interactions.copy()
			inter['date'] = pd.to_datetime(inter['time']).dt.date
			affected = inter[inter['dst_user'].isin(flag_authors)]
			reduced_spread = int(len(affected) * 0.5)  # 假设干预减少50%扩散
		else:
			reduced_spread = 0

		report = {
			'total_flagged': int(flagged.shape[0]),
			'unique_authors_flagged': int(len(flag_authors)),
			'estimated_spread_reduction': int(reduced_spread),
			'threshold': float(self.threshold),
			'max_daily_flags': int(self.max_daily_flags),
		}
		return report
```

**干预机制**：
1. **阈值标记**：预测概率≥阈值的内容标记为高风险
   - **社会计算对应**：基于**风险评分**的决策规则 $flag(c) = \mathbb{I}[P(y_c=1|X_c) \geq \theta]$
2. **每日上限**：限制每日最大处置数量（避免过度干预）
   - **社会计算对应**：资源约束 $\sum_{d} |F_d| \leq M$，其中 $F_d$ 是第 $d$ 天标记的内容集合
3. **作者冷却**：对被标记作者施加冷却期，减少其内容的传播扩散
   - **社会计算对应**：冷却机制基于**社会影响最小化**理论，通过限制高风险用户的传播能力来抑制信息扩散
4. **效果估算**：假设干预可减少50%的扩散量
   - **社会计算对应**：扩散抑制模型 $reduced\_spread = |affected\_interactions| \times 0.5$

**输出报告**：
- 标记内容总数
- 被标记作者数
- 估计减少的扩散量
- 使用的阈值和每日上限

---

## 六、评估与可视化模块

### 6.1 社会计算理论基础：模型评估与性能分析

**理论背景**：在社会计算中，模型评估需要综合考虑**分类性能**和**实际应用效果**。对于不平衡分类问题（如诈骗检测），传统的准确率指标不够充分，需要使用**ROC曲线**和**PR曲线**等更合适的评估方法。

**形式化定义**：
- **ROC曲线**：以假正率（FPR）为横轴，真正率（TPR）为纵轴的曲线
  - FPR = $\frac{FP}{FP + TN}$，TPR = $\frac{TP}{TP + FN}$
  - **ROC AUC**：ROC曲线下面积，衡量模型区分正负样本的能力
- **PR曲线**：以召回率（Recall）为横轴，精确率（Precision）为纵轴的曲线
  - Precision = $\frac{TP}{TP + FP}$，Recall = $\frac{TP}{TP + FN}$
  - **PR AUC**：PR曲线下面积，更适合不平衡分类问题

**社会计算知识点**：
- **不平衡分类评估**：在诈骗检测中，正样本（诈骗）通常占少数，PR AUC比ROC AUC更能反映模型在实际应用中的性能
- **阈值选择**：ROC/PR曲线帮助选择最优分类阈值，平衡精确率和召回率
- **干预效果评估**：通过统计标记内容数、被标记作者数和估计减少的扩散量来评估干预策略的实际效果

### 6.2 `src/evaluation/evaluate.py` - 评估器

**功能概述**：可视化模型性能指标，输出评估报告。

**核心类**：`Evaluator`

#### 6.2.1 ROC/PR曲线绘制

```7:38:src/evaluation/evaluate.py
	def plot_roc_pr(self, metrics: Dict[str, Any], save_path: str):
		fpr, tpr = metrics.get('roc_curve', ([], []))
		prec, rec = metrics.get('pr_curve', ([], []))

		if not fpr or not tpr or not prec or not rec:
			# 单类情况：输出说明性图片
			fig = plt.figure(figsize=(6, 3.2))
			txt = metrics.get('note', 'single-class; ROC/PR unavailable')
			plt.axis('off')
			plt.text(0.02, 0.6, f"ROC/PR 无法计算\n原因: {txt}", fontsize=12)
			fig.tight_layout()
			fig.savefig(save_path, dpi=150)
			plt.close(fig)
			return

		fig, axs = plt.subplots(1, 2, figsize=(10, 4))
		axs[0].plot(fpr, tpr, label=f"ROC AUC={metrics['roc_auc']:.3f}")
		axs[0].plot([0, 1], [0, 1], '--', color='gray')
		axs[0].set_title('ROC')
		axs[0].set_xlabel('FPR')
		axs[0].set_ylabel('TPR')
		axs[0].legend()

		axs[1].plot(rec, prec, label=f"PR AUC={metrics['pr_auc']:.3f}")
		axs[1].set_title('PR')
		axs[1].set_xlabel('Recall')
		axs[1].set_ylabel('Precision')
		axs[1].legend()

		fig.tight_layout()
		fig.savefig(save_path, dpi=150)
		plt.close(fig)
```

- 绘制ROC曲线和PR曲线（并排显示）
  - **社会计算对应**：ROC曲线反映模型在不同阈值下的**分类性能**，PR曲线更适合评估**不平衡分类**问题
- 显示AUC值
  - **社会计算对应**：AUC值提供模型性能的单一数值指标，便于模型比较和选择
- 单类别情况特殊处理（显示说明文字）
  - **社会计算对应**：当数据集中只有单一类别时，使用无监督异常检测（IsolationForest），此时ROC/PR曲线不适用

#### 6.2.2 评估报告汇总

```40:48:src/evaluation/evaluate.py
	def summarize(self, metrics: Dict[str, Any], intervention_report: Dict[str, Any]):
		if metrics.get('roc_auc') is None:
			print("单一类别子集：使用无监督异常检测，ROC/PR 不适用。")
		else:
			print(f"ROC AUC: {metrics['roc_auc']:.4f}")
			print(f"PR  AUC: {metrics['pr_auc']:.4f}")
		print("干预报告:")
		for k, v in intervention_report.items():
			print(f"- {k}: {v}")
```

- 输出模型性能指标（ROC AUC、PR AUC）
  - **社会计算对应**：这些指标反映模型在**不平衡数据集**上的整体性能，是评估诈骗检测系统的重要指标
- 输出干预策略报告
  - **社会计算对应**：干预报告包含标记内容数、被标记作者数和估计减少的扩散量，用于评估**干预策略的实际效果**

---

## 七、主流程整合（main.py）

### 7.1 社会计算理论基础：端到端系统流程

**理论背景**：在社会计算中，完整的诈骗检测系统需要整合**数据获取**、**特征工程**、**模型训练**、**干预策略**和**效果评估**等多个环节。这种端到端（End-to-End）的设计遵循**社会计算系统架构**理论。

**系统流程**：
1. **数据层**：数据模拟/加载 → 数据清洗 → 数据标准化
2. **图计算层**：构建社交图 → 计算社会信号
3. **特征层**：多模态特征提取 → 特征融合
4. **模型层**：模型训练 → 预测 → 评估
5. **干预层**：风险评分 → 阈值决策 → 干预执行
6. **评估层**：性能评估 → 可视化 → 报告输出

**社会计算知识点**：
- **模块化设计**：各模块职责清晰，便于维护和扩展
- **配置驱动**：通过YAML配置文件统一管理参数，支持实验复现
- **多数据集支持**：支持模拟数据和真实数据集（UCI），提高系统通用性

### 7.2 完整流程

```33:106:main.py
def main():
	parser = argparse.ArgumentParser() #创建一个参数解析器
	parser.add_argument('--config', type=str, default='configs/base.yaml')
	parser.add_argument('--dataset', type=str, default='simulated', choices=['simulated', 'uci_phishing', 'sms_spam', 'twitter_accounts'])
	parser.add_argument('--uci_dir', type=str, default='.')
	parser.add_argument('--twitter_dir', type=str, default='.')
	parser.add_argument('--twitter_subset', type=str, default=None, choices=[None, 'verified', 'botwiki'])
	args = parser.parse_args() #解析用户在命令行中输入的参数，并存入 args 对象中

	ensure_dirs()
	if not Path(args.config).exists():
		# 默认配置
		default_cfg = {
			"simulation": {"num_users": 2000, "fraud_ratio": 0.08, "seed": 42},
			"features": {"ngram_range": [1, 2], "max_features": 5000},
			"model": {"type": "xgb_like", "params": {"n_estimators": 300, "learning_rate": 0.08}},
			"intervention": {"threshold": 0.7, "cooldown_days": 7, "max_daily_flags": 100},
			"evaluation": {"test_size": 0.25, "random_state": 42}
		}
		Path(args.config).write_text(yaml.safe_dump(default_cfg, allow_unicode=True), encoding='utf-8')

	cfg = load_config(args.config)

	# 1) 读取数据
	if args.dataset == 'simulated':
		sim_data = simulate_social_platform(**cfg["simulation"])
		ds_name = 'simulated'
	elif args.dataset == 'uci_phishing':
		sim_data = load_uci_phishing(args.uci_dir)
		ds_name = 'uci_phishing'
	elif args.dataset == 'sms_spam':
		sim_data = load_uci_sms_spam(args.uci_dir)
		ds_name = 'sms_spam'
	elif args.dataset == 'twitter_accounts':
		sim_data = load_twitter_accounts(args.twitter_dir, subset=args.twitter_subset)
		ds_name = f"twitter_{args.twitter_subset or 'all'}"
	else:
		raise ValueError('unknown dataset')

	# 2) 构建整合数据集
	df_users, df_contents, df_interactions, df_labels = build_dataframe(sim_data)

	# 3) 构建社会图并计算社会信号
	G = build_social_graph(df_users, df_interactions)
	social_signals = compute_social_signals(G, df_users, df_contents, df_interactions)

	# 4) 特征工程（融合文本/用户画像/网络/时间/社会信号）
	extractor = FeatureExtractor(cfg["features"]) #初始化特征提取器
	X, y, meta = extractor.transform(
		df_users=df_users,
		df_contents=df_contents,
		df_interactions=df_interactions,
		df_labels=df_labels,
		social_signals=social_signals,
	)

	# 5) 训练检测模型
	detector = FraudDetector(cfg["model"]) #初始化欺诈检测器，传入模型配置
	clf, metrics, (test_idx, y_test, y_pred_proba) = detector.fit_and_eval(X, y, cfg["evaluation"])
	#调用此方法，该方法内部会划分训练集/测试集,在训练集上训练模型 clf，并在测试集上进行评估
	#返回clf:训练好的分类器对象,metrics:包含评估指标,(test_idx, y_test, y_pred_proba): 测试集的索引、真实标签和模型预测的概率

	# 6) 干预策略仿真（对齐测试集样本）
	meta_test = meta.iloc[test_idx].reset_index(drop=True)
	simulator = InterventionSimulator(cfg["intervention"])
	intervention_report = simulator.simulate(meta_test, y_pred_proba, df_interactions)

	# 7) 评估与可视化
	evaluator = Evaluator()
	save_path = f'outputs/metrics_{ds_name}.png'
	evaluator.plot_roc_pr(metrics, save_path=save_path)
	evaluator.summarize(metrics, intervention_report)

	print(f"流程完成。指标图已输出至 {save_path}")
```

**执行流程**：
1. **数据加载**：支持模拟数据或UCI数据集
2. **数据清洗**：构建标准数据框
3. **图计算**：构建社交图，计算社会信号
4. **特征提取**：多维度特征融合
5. **模型训练**：训练检测模型并评估
6. **干预仿真**：模拟干预策略效果
7. **结果输出**：可视化评估指标和干预报告

---

## 总结

本系统实现了从数据生成/加载、特征工程、图计算、模型训练、干预仿真到评估可视化的完整流程。各模块职责清晰，通过配置文件统一管理参数，支持多种数据集和模型选择，适用于金融诈骗检测的实验研究。

